# train_hyperimpute_min.py
import os, pandas as pd
import numpy as np
from tqdm import tqdm
import argparse
from hyperimpute.plugins.imputers import Imputers
from dataset import Preprocessor, get_eval
from generate_mask import generate_mask
from hyperimpute.plugins.utils.metrics import RMSE
from hyperimpute.plugins.utils.simulate import simulate_nan
from sklearn.metrics import mean_squared_error, root_mean_squared_error
from hyperimpute.utils.serialization import load, save
import torch

import warnings
warnings.simplefilter("ignore", FutureWarning)

# --- stability on macOS / BLAS ---
for k in ["OMP_NUM_THREADS", "OPENBLAS_NUM_THREADS", "MKL_NUM_THREADS",
          "VECLIB_MAXIMUM_THREADS", "NUMEXPR_NUM_THREADS",
          "CATBOOST_THREAD_COUNT", "XGB_NUM_THREADS"]:
    os.environ[k] = "1"
os.environ["KMP_INIT_AT_FORK"] = "FALSE"
parser = argparse.ArgumentParser(description='Missing Value Imputation')

parser.add_argument('--dataname', type=str, default='adult', help='Name of dataset.')
parser.add_argument('--gpu', type=int, default=0, help='GPU index.')


args = parser.parse_args()
# check cuda
if args.gpu != -1 and torch.cuda.is_available():
    args.device = f'cuda:{args.gpu}'
else:
    args.device = 'cpu'


# Generate mask based on the amputation mechanism
# def ampute(x, mechanism, p_miss):
#     x_simulated = simulate_nan(np.asarray(x), p_miss, mechanism)
#
#     mask = x_simulated["mask"]
#     x_miss = x_simulated["X_incomp"]
#
#     return pd.DataFrame(x), pd.DataFrame(x_miss), pd.DataFrame(mask)


if __name__ == "__main__":
    torch.manual_seed(42)
    np.random.seed(42)
    # Initialize other args
    dataname = args.dataname
    device = args.device
    models_dir = f'saved_models/{dataname}/'

    train = pd.read_csv(f"datasets/{dataname}/train.csv")  # source train data file
    test = pd.read_csv(f"datasets/{dataname}/test.csv")  # source test data file

    # prepare the data: train and test data (true), test data (with missing values), and the mask
    prepper = Preprocessor(dataname)
    train_X = prepper.encodeDf('Ordinal', prepper.df_train)  # train_X is a numpy array
    test_X = prepper.encodeDf('Ordinal', prepper.df_test)  # test_X is a numpy array
    num_numeric = prepper.numerical_indices_np_end  # index of the last numeric column

    mean_X, std_X = (
        np.mean(train_X, axis=0), np.std(train_X, axis=0)
    )

    in_dim = train_X.shape[1]
    X = (train_X - mean_X) / std_X
    # X = torch.tensor(X)
    X_test = (test_X - mean_X) / std_X
    # X_test = torch.tensor(X_test, dtype=torch.float32)

    # test_X_ori_fmt = np.concatenate((prepper.df_test.iloc[:, prepper.info['num_col_idx']],
    #                                  prepper.df_test.iloc[:, prepper.info['cat_col_idx']]), axis=1)
    # test_X_ordinal_fmt = pd.DataFrame(prepper.encodeDf('Ordinal', prepper.df_test))

    # Keep original index if you have a DF you transformed
    # idx = test_X_ordinal_fmt.index if isinstance(test_X_ordinal_fmt, pd.DataFrame) else None

    # df_out = pd.DataFrame(test_X_ordinal_fmt, index=idx)
    # out_path = os.path.join(models_dir, "test_X_ordinal_fmt.csv")
    # df_out.to_csv(out_path, index=False)

    mask_type = 'MCAR'  # or 'MAR', 'MCAR', 'MNAR_logistic_T2'
    ratio = 0.2  # train on MCAR 0.2

    train_mask = generate_mask(train_X, mask_type=mask_type, mask_num=1, p=ratio)[0]
    # test_mask = generate_mask(test_X, mask_type, mask_num=1, p=0.25)[0]
    # test_masks = prepper.extend_mask(orig_mask, encoding='OHE')

    # print("Original Mask:", orig_mask.shape, orig_mask[:5])
    # print("Test Masks:", test_masks.shape, test_masks[:5])

    # exit()

    # 2) Build the imputer
    plugins = [
        "hyperimpute",  # or
        "miracle",
        "gain"
        # "mean", "median", "knn", "mice", "missforest", "softimpute"
    ]

    for plugin in plugins:
        print(f"Plugin: {plugin}")
        if plugin == "hyperimpute":
            imputer = Imputers().get(
                plugin,
                random_state=42,
                optimizer="hyperband",  # "simple" is faster, "bayesian" more thorough
                # n_jobs=1,                       # keep single-threaded for stability
                classifier_seed=["logistic_regression", "random_forest", "xgboost", "catboost"],
                regression_seed=["linear_regression", "random_forest_regressor", "xgboost_regressor",
                                 "catboost_regressor"],
                # class_threshold=5,  # ≤5 uniques → treat as categorical
                # imputation_order=2,  # random order across columns
                # n_inner_iter=10,
                # select_model_by_column=True,
                # select_model_by_iteration=True,
                # select_patience=5,
            )
        else:
            imputer = Imputers().get(plugin, random_state=42)  # use Imputers to get the model
        # 3) Fit on training data (learn how to fill)
        X_miss = X.copy()
        X_miss[train_mask] = np.nan
        imputer.fit(X_miss)
        # Save to a file

        buff = save(imputer)  # get the model as bytes
        # X_miss_test = X_test.copy()
        # X_miss_test[test_mask] = np.nan
        #
        # imputed_train = imputer.transform(X_miss).values
        # # exit()
        # imputed_train_destandardized = imputed_train.copy()
        # imputed_train_destandardized[:, num_numeric:] = ((imputed_train * std_X) + mean_X)[:, num_numeric:]
        # decoded_imputed = prepper.decodeNp('Ordinal', imputed_train_destandardized)
        # X_destandardized = X.copy()
        # X_destandardized[:, num_numeric:] = ((X * std_X) + mean_X)[:, num_numeric:]
        # X_destandardized_decoded = prepper.decodeNp('Ordinal', X_destandardized)
        # decoded_imputed[~train_mask] = X_destandardized_decoded[~train_mask]
        # mse, acc = get_eval(decoded_imputed, X_destandardized_decoded, train_mask, num_numeric)
        # print(f'{plugin}, MSE: {mse}, ACC: {acc}')
        os.makedirs(models_dir, exist_ok=True)
        path = os.path.join(models_dir, f"{plugin}.pkl")

        # after imputer.fit(...)
        with open(path, "wb") as f:
            f.write(buff)  # .model() returns bytes
        # with open(f"{models_dir}/hyperimpute_{plugin}_final.pkl", "wb") as f:
        #     f.write(imputer.save())  # imputer.save() returns a bytes object
        print(f"Saved imputer model: {f.name}")

        # exit()  # exit after saving the model

        # 4) Impute
        # test_x_true, test_x_missing, test_x_mask = ampute(test_X_ordinal_fmt, mask_type, ratio)  # simulate missing data
        # train_imp = imputer.transform(train)
        # test_imp = imputer.transform(test_x_missing.copy())  # impute the missing data

        # 5) Save artifacts to check the results later
        # train_imp.to_csv("train_x_imputed.csv", index=False)
        # test_imp.to_csv(f"{models_dir}test_x_imputed_{plugin}.csv", index=False)
        # print("test imputed:", pd.DataFrame(test_imp))

        # mse = root_mean_squared_error(test_x_true.values, test_imp.values)  # RMSE
        # rmse = RMSE(test_imp.values, test_x_true.values, test_x_mask.values)
        # print(f"Plugin: {plugin}, RMSE: {rmse}, {mse * mse}")

        # print(f"Saved: test_x_original.csv, test_x_imputed.csv, hyperimpute_model_{plugin}.pkl")
